\chapter{Introduction}
Recent advances in the technology of automated theorem proving (ATP) and abstract interpretation allow the automated verification of increasingly large and complex programs.
Fully automated verification tools can annotate a program with loop and recursion invariants and verify that the annotated program is correct according to a given specification. 

In this thesis we are interested in verifying the correctness of (manually or automatically) annotated programs rather than the inference of invariants - this verification problem is interesting in itself when user-given annotations are available and is commonly a sub-problem for more automated tools that try to verify approximations of the program (such as bounded loop unrolling) and approximations of the annotation (such as loop invariants generated by abstract interpretation or by interpolation on the proof of an unrolled program).

Popular program verification techniques (for annotated programs) include variants of verification condition (VC) generation (VCG) and symbolic execution.

In VCG based verification, an annotated program and a specification are encoded in a mathematical formula that holds iff the program satisfies the specification, as pioneered by the work of Dijkstra with weakest preconditions (\cite{Dijkstra:1975:GCN:360933.360975}). This formula can be fed to an automated theorem prover. VCG allows the use of general first order logic (FOL) theorem provers, whether based on SMT solving (e.g. \cite{DBLP:conf/cav/BarrettCDHJKRT11},\cite{DBLP:conf/tacas/MouraB08}), completion (\cite{HJL99},\cite{DBLP:conf/cade/RiazanovV99}) instantiation (\cite{Korovin2008}), or other (\cite{BaumgartnerPelzerTinelli12}), although SMT solvers are, by far, the most common. 
One of the main disadvantages of VCG is that the translation to FOL loses some explicit information about the program which can be useful for the proof search, such as the control structure of the program and the scope of variables.

Symbolic execution simulates executing the program on each possible path, using symbolic values rather than concrete values for variables. The symbolic execution engine evaluates the feasibility of a program path by using a constraint solver (often a SAT or SMT solver) to check the satisfiability of the branch conditions on a path with the calculated symbolic values - hence, essentially, several small VCs are sent to the solver, each for a specific path in the program. Symbolic execution tools are sensitive to the problem of path explosion - the number of feasible paths in a program can be exponential in the size of the program, even though the reasoning needed for proving two non-disjoint paths can be very similar.

SMT solvers have made significant progress in recent years, and are quite efficient and reliable in solving problems that involve ground FOL combined with other ground theories, notably linear integer and rational arithmetic and arrays. However, when the VC includes quantified formulae, SMT solvers are, in general, not complete, and the performance and even termination of the solver is very sensitive to the input formula, even to parts that are logically unrelated to the proof (as noted e.g. in \cite{LeinoP16}).
Quantified VCs are needed for modeling abstractions such as sets and sequences used in specifications, for encoding some invariants and for modeling used in some verification methodologies, such as permissions or dynamic frames for alias control.

The instability of SMT solvers in the presence of quantifiers makes verifiers based on SMT solvers less accessible even to people that are familiar with program proofs, specification and first order logic - users need understanding of the working of the SMT solver in order to understand why verification fails, especially for a program that previously verified and has been modified only slightly. 
In addition, as FOL is only semi-decidable, any theoretically complete tool will not be bounded in its run-time.
For users, it is sometimes useful to know that if a proof for a property of the program was found using a certain effort of the verifier, and the program has been modified but the proof of the property carries over to the modified program, then the property will be proven if the same effort is spent by the verifier on the modified program, regardless of other properties and unrelated modifications of the program. Classic resource limitations on the prover, such as memory and time, cannot usually satisfy this requirement.
In addition, if some of the properties of a program are easy to prove while others require longer time or do not hold, it is useful to show the user of a verification tool intermediate results, such as proven properties, while still searching for proofs for the harder properties. Such intermediate results can also be used to cooperate between verification and analysis tools. While the verifier can be run for each property separately, this is often highly inefficient as the proofs of different properties of the same program often share a large number of lemmas.

ATPs based on superposition (e.g. \cite{DBLP:conf/cade/RiazanovV99}) handle quantifiers efficiently, 
and, in addition, can be modified to search incrementally for proofs of increasing complexity - for example, the proof depth can be bounded (by blocking inferences of maximal depth). Such ATPs have seen much less use in program verification as superposition is not very efficient in handling ground and propositional formulae, which are often the majority of the VC for program verification. In addition, the extensions suggested for superposition based solvers to support linear arithmetic are not as efficient as those for SMT solvers, and integer arithmetic is often needed for program verification.

Program analysis tools based on abstract interpretation calculate an over-approximation of the set of feasible program states at each program point, by applying an abstraction to the state and transition relation of the program.
If the approximation satisfies the specification for that program point, so does the program.
For a given abstraction, it is sometimes possible for a programmer to predict which properties of a program will be proved, and the approximation at each program point is not affected by modifications to disjoint parts of the program - hence the results of the tool are more stable and predictable to non-expert users. Abstract interpretation can be applied in abstractions of increasing strength, so that the user can expect that properties proven in a certain abstraction will be proven with the same abstraction if the proof is valid for the modified program.
Very few abstract interpreters were suggested for quantified domains (e.g. \cite{DBLP:conf/popl/GulwaniMT08}),
and they often have to lose precision at join points as they are not goal sensitive - the analyzer at a join point cannot predict what precision is needed to prove properties later in the program.

\textbf{Main contribution:}\\
In this thesis we propose a generic verification algorithm that is based on a tighter integration between theorem proving and verification. 
Our algorithm is based on the idea of having a local theorem prover at each program point rather than one global prover, and allowing these provers to exchange information in order to search for a proof for the entire program. Information is exchanged only on-demand between provers rather than eagerly as often in abstract interpretation.
Our algorithm is incremental and applies successively stronger logical fragments in order to prove a program, allowing the report of intermediate results to the user. For a given fragment, the proof of each property is independent from the proofs of other properties, but the proofs can still share lemmas. 

We have instantiated our algorithm for the fragment of ground equalities using a form of congruence closure graphs, and for general FOL using superposition. We have also implemented a hierarchy of bounded fragments that restrict the proof-tree shape in various ways, including the size of terms and proof depth.

We show how our algorithm can take advantage of the scoping inherent in many programming languages, 
so that the vocabulary of each prover is small and local - we show how to preserve completeness under scoping using interpolation for ground FOL.

As we have implemented only very basic support for linear integer arithmetic, our tool cannot prove many VCs on its own. 
However, as intermediate results are usable at any stage, 
we can use the tool as an optimizing pre-processor before running an SMT solver or other tool.

%The essence of SMT solvers works by exploring the decision tree of the input formula (assigning the propositional value of each input literal) and learns new information only by reaching a conflict - a branch of the decision tree which is an assignment under which the input formula evaluates to false. 
%In ground FOL the depth of the decision tree is bounded by the size of the input formula and the input formula is guaranteed to evaluate to true or false under a full assignment. In FOL, the depth of the decision tree is not bounded - each \emph{ground instance} of each input literal must be decided. 
%SMT solvers often operate on only the ground subset of the axioms in the input formula and use some heuristics to add ground instances of the quantified axioms to the clause set at various points of the algorithm - however, there is no guarantee that any amount of work will allow the solver to progress, even if resource limitations and restarts are used. In addition, checking if an assignment falsifies the input formula is only semi-decidable.




%
%
%Regardless of the ATP used, the generation of a VC for a program in FOL often loses some information, such as the scope of program variables, and makes some information implicit, such as the structure of the program control flow graph (CFG). 
%Compilers and static analysis tools are able to take advantage of some of this information for a program in order to infer information that is usable in simplifying and optimizing the program, and in reporting errors to the programmer. A theorem prover often learns several lemmas during the proof-search for a VC, which could be useful for simplifying the VC by a VCG tool or a static analysis tool with knowledge of the program structure, but cannot be used as efficiently by the prover itself because of the loss of structure.\\
%Modern theorem provers, and especially SMT solvers, can solve an increasing number of problems efficiently, 
%but tend to be unpredictable in terms of run-time and even termination when quantifiers are involved. Very small changes in the input formula can have a significant effect on the performance of the prover. This is in stark contrast to compilation tools, which users expect to perform more consistently, and hence reduces the appeal of some verification tools.\\
%The VC of a program often includes many small properties to prove, such as a variable not being equal to null at some program point.
%It is often the case that the proof of many of these properties can be found quickly, but if one of the properties is hard to prove or does not hold and a counter-example is hard to find, a prover run on the VC will not give the user any information about the provable properties. While it is possible to generate a separate VC for each property, it is often the case that different properties share a significant part of the proof, and hence verifying each property separately implies much duplication of work. In addition, for a program VC that contains quantified formulae, when proving one property, adding or removing an unrelated property, without any modification to the program and hence the validity of the first property, can make the prover succeed or time-out on the first property, because it can modify the search order of the prover.\\
%Partial verification results, such as a proof of some of the properties or the infeasibility of some program points can be useful for a verification tool user even while the proof-search for a harder property continues. Such intermediate results are useful for immediate feedback to the user but also can allow further optimizations of the program VC and can be communicated to another static analysis or verification tool.

%During the transformation from a program to a FOL formula, some information is lost or becomes implicit - for example, the graph shape of the CFG is often encoded into the propositional part of the FOL formula and so some graph properties such as reachability in the CFG and variable scoping are only implicitly encoded. 
%Furthermore, a specification for a program often includes many different properties to be proven. A tool can either encode the VC for each property into a separate FOL formula or it can encode one VC that is valid only if all properties hold. In practice, it is often the case that the proofs of many properties of the same program share a large part and so encoding one VC is usually more efficient. 
%However, if one property takes very long to prove, or does not actually hold, we are left with no information about the other properties we are trying to prove, even if they were actually proven during the theorem prover run. 
%In addition, as the verification problem is generally undecidable in the presence of quantified axioms and at least of worst-case exponential complexity otherwise, a failed verification run (either time-out or unknown answer) does not give us any information and any work done by the prover is generally lost. Users of a verification tool will expect some consistency, where a property that has been proven by the verifier will be proven again with the same parameters even if further properties have been added to the specification, which is not always the case when quantifiers are involved.
%Additionally, both theorem provers and verifiers often rely on pre-processing of the original formula and program, respectively for performance. Once several properties have been proven, the formula sent to the prover can be simplified further in order to improve the performance of the prover on other properties, but this cannot be performed when the VC is sent only once to the prover.


%In order to address the above issues we propose a tighter integration between the verifier and the theorem prover.
%We interleave steps of the theorem prover and steps of the verifier and direct the prover so that it searches for a proof in logical fragments of increasing strength and predictable worst-case time, so that a property that is provable in a certain fragment will be proven in a given time regardless of the rest of the program and other properties. 
%Working with a hierarchy of logical fragments allows users to predict which properties should be provable at which stage, increasing the reliability of a verification tool.
%We have chosen to specialize for verification a superposition based prover rather than an SMT based prover as superposition based provers are better suited for handling quantifiers, and are more suitable for working incrementally in logical fragments of increasing strength, although they are weaker at handling theories such as integers. One of the main weaknesses of superposition based provers is the handling of the propositional part of a formula, in which SMT provers based on CDCL (Conflict driven clauses learning \cite{GRASP}) excel - however, in our experience, the majority of the propositional part of a VC formula often comes from the encoding of the CFG, which is handled by the verifier rather than prover in our case.
%Our verification algorithm essentially annotates each basic block of the program CFG with a formula describing the post-state of that block, using increasingly strong logical fragments to strengthen the annotation, and some optimization techniques to simplify the CFG based on the annotation. As the correlation with the original program structure is preserved, our tool can cooperate with other tools at any stage.

%Other approaches for the verification and inference of program properties use theorem provers (mostly SAT or SMT solvers) as a tool for answering FOL satisfiability queries, and generate many smaller independent prover calls during the verification of a program (e.g. \cite{AlbarghouthiLGC12}, \cite{Bradley12}, \cite{McMillanRybalchenko2013}). These approaches use the structure of the program to direct the verification and determine the calls made to the prover. Another well established approach to inferring and proving program properties is that of abstract interpretation (\cite{CousotCousot77}) which exploits the program structure and works in a selected fragment of the logic used for program semantics and hence can guarantee the verification of all properties that are provable within the fragment and can enforce bounds on the run-time of the tool.





%A verification algorithm for computer programs takes as input a program in some programming language with precisely defined semantics and a specification in some specification language which is also precisely defined, and outputs whether the program satisfies the specification or not. As the problem of whether a program satisfies a specification is not decidable in most programming and specification languages, sound verification algorithms will either not terminate or return unknown for some input programs.
%The earliest work by Floyd and Hoare (\cite{FLOYD67},\cite{DBLP:journals/cacm/Hoare69}) introduced Hoare-triples for specification, 
%using logical formulae (with a vocabulary that includes the program variables as logical constants) to describe the state at each program point, and an inference system to derive 
%
%
 %describe the set of pre- and post-states of a statement as formulae in first order logic (FOL) with a signature that includes integer arithmentic. of executing each statement of the program. The set of states in a pre- or post-state is described as a formula in some specification logic, which is usually first order logic (FOL) with a signature that includes integer arithmetic. They introduced a system of axioms and inference rules that allows the derivation of any pair of pre- and post-state that holds in the program, if we can derive any consequence of a formula in the specification logic (under some assumptions about the specification logic - \cite{DBLP:journals/siamcomp/Cook78}).
%A program Given a program in some programming language and semantics for the language
%Given a program and a specification, a viable method for verifying that the program satisfies the specification is the generation of verification conditions (VCs) and feeding these to an automated theorem prover (ATP) for first order logic (FOL) with background theories.
%This process allows the use of off-the-shelf general purpose ATPs such as SMT solvers and superposition based provers, but naturally a lot of the information explicit in the program is lost or made implicit in the conversion to standard FOL. Also, unless the VC generation is done very carefully, a lot of redundancy is introduced (e.g. the classic weakest precondition of a branch statement), and even with careful VC encodings, much redundancy can still exist in the proof search for common ATP architectures (e.g. SMT).
%In following the original formulation of program semantics by Hoare, the theorem prover is needed only for the application of the rule of consequence. 
%Often this means that the scope and search space for each such proof is much smaller than for the whole program, and as ATPs are quite sensitive to the input size, we try to construct a correctness proof for the program by applying an instance of a theorem prover per program point, and using the control flow graph (CFG) structure, variable scope and other properties explicitly to search which Hoare triples are needed for the application of the other Hoare rules.
%
%\section{Proof Direction}
%VC generation usually uses some variation of weakest precondition (WP) and weakest liberal precondition (WLP).
%This generally treats the program "backwards" (from assertions to entry point).
%Running an SMT solver on such an encoding actually treats the program forward - so the actual decision tree of the SMT solver includes the tree of control flow paths of the program (usually without a way to join information from different paths at a CFG join point).
%The advantage of reasoning forward is that most assignments are much easier to handle forwards, while the disadvantage is that a lot of unnecessary information is generated. On the other hand reasoning backwards is more "goal directed" but can create a lot of redundancy.
%We are interested in the flow of information along the CFG, and try to make it optimal by propagating information (equalities, clauses etc) forward, but only on demand. The relevant clauses to propagate forward are the ones that are potential candidates for resolution/superposition inference - the idea is that any such inference should happen in the earliest point in the program where all the premises hold, but only if it is (potentially) needed to prove some assertion (later) in the program.
%For equalities we have a dedicated congruence closure mechanism that can handle joins, but again only propagates ground terms on demand - and again congruence closure happens at the earliest point an equality is known to hold and so is incremental along the CFG.
%By forcing inferences to the earliest possible point we maximize the potential for sharing information between branches - which is often missed by running a general theorem prover.
%
%\section{Control Flow Graph}
%Program compilation and optimization makes heavy use of properties of CFGs, and can rewrite CFGs to improve program performance.
%We try to take advantage of this in order to optimize the program for verification.
%We make heavy use of Floyd's rule of conjunction, so that any (FOL) clause proven at some program point can be taken forward to a later program point (in a passive program). We try to generate and propagate these clauses on demand basis, only considering those that have a chance to participate in a resolution or superposition inference.
%We have a special treatment for branch join points, with a dedicated algorithm for joining equalities, inequalities and unit clauses (and to a lesser extent general clauses)  - this can be a weak point for SMT based ATPs as it usually incurs a lot of redundancy in the proof.\\
%Having explicit CFG information means some of the case-splitting in the proof is done directly following the branches in the program. When this is insufficient, we try to have a kind of case-splitting on demand - when we reach a point in the program where we detect that a case split would give more information about a clause/ground term that is deemed relevant, we can calculate (incrementally) the extra clauses implied by this case split, and sometimes even add the case split directly to the CFG.
%This means we are actually integrating some of the techniques of SMT solvers into a mainly resolution based prover.
%
%\section{Scope}
%Many programming languages have scoped variables, which we can use to our advantage by reducing the signature of the ATP instance in each program point to only program variables that are in scope at that point.
%We transform the program to dynamic single assignment (DSA) form, and can further reduce the signature at a given program point by discarding older DSA versions of program variables that are representable as terms over yet older versions of the variable - thus dramatically reducing the signature of each ATP instance without compromising completeness.
%Using a strategy akin to Set of Supports (assuming the axioms are consistent), we can also reduce the number of relevant axioms and deduction steps per ATP instance.
%
%\section{Proof Depth}
%A common property of computer program correctness proofs is that they are big but shallow - there are many resolution proof steps, but most of the proof is on ground terms (most Hoare triples are ground) and the actual ground terms that appear in the proofs are not very "far" from those that appear in the program - as opposed to e.g. algebra or calculus proofs with elaborate constructions.
%We try to take advantage of this property by restricting resolution and superposition applications to only those that produce terms at a certain distance from the original program (by a metric we define) - this restricts the search space quite a bit and the radius can always be increased iteratively to get a semi-decision procedure.
%
%\section{Heaps}
%Heaps are often encoded into FOL as update-able maps, and the theory has either a builtin decision procedure or is axiomatized (there are complete axiomatizations). However, we can further benefit also from the specific knowledge that each heap assignment creates a new DSA version of the heap and of having a dedicated procedure for handling joins in the CFG. 
%For heap manipulating programs, this distance property means that, in the proof, we only consider heap locations that are near (reachable in a limited number of heap dereferences) to those that appear in the program - this is especially useful for modular verification methodologies that derive global heap properties from local verification conditions.
%For programs using recursive data structures with recursive predicates, this distance property correlates directly with limiting the number of unfoldings of predicates on heap nodes.
%
%\section{Summary}
%We are attempting to specialize a resolution theorem prover to proving programs, taking advantage of the specific properties of programs and program proofs and especially modular verification condition generation techniques.


\section{Outline}
In the rest of this chapter we give an overview of the main ideas in this thesis.
In chapter \ref{chapter:preliminaries} we discuss the theoretical background that we assume, our notation and the structure of input programs that we can handle.
In chapter \ref{chapter:ugfole} we present our verification algorithm and instantiate it for unit ground equalities using a form of congruence closure graphs.
In chapter \ref{chapter:gfole} we instantiate our algorithm for ground clauses using ground superposition.
In chapter \ref{chapter:scoping} we show how our algorithm is adapted to search for local proofs, and the use of interpolation to preserve completeness for local proofs in the ground case.
Chapter \ref{chapter:bounds} introduces several restrictions on the shape of proofs that define a hierarchy of decidable logical fragments with predictable complexity, whose limit is the complete fragment of first order logic, thus allowing incremental verification in fragments of increasing strength.
We show how the algorithm developed for ground clauses is extended to quantified clauses in chapter \ref{chapter:quantification}.
In chapter \ref{chapter:implementation} we discuss some implementation issues and present experimental results for an implementation of some of the ideas in this thesis.
We conclude in chapter \ref{chapter:conclusions} and discuss future work.

\input{main_ideas}
