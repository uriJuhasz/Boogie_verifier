\chapter{Bounded fragments}\label{chapter:bounds}
In this section we present several restrictions of the various fragments we have described that allow us to ensure polynomial space and time bounds.
Our bounds are based on restricting the size of each term, literal or clause according to several different measures of size, and also the derivation depth.\\
Our objective is to have a predictable verification tool, with a predictable maximal run-time for a given selection of parameters and, given a proof for a property of the program, a simple check whether the proof lies within the bound defined by the set of parameters - regardless of random seeds, processor performance etc. For this reason all of our bounds are independent of order of evaluation - for example, we do not restrict the total number of terms or clauses, only their structure.
For several of the fragments we only show that the maximal time and space are polynomial, without showing the exact degree, which remains as future work.

\section{Term depth}
One of the simplest ways to limit the size of a term is to limit the maximal term depth.
For a finite signature with at least one function of more than unary arity, 
the maximal size of a term is exponential in the maximal term depth.
The other factors are the number of functions, function arities (especially maximal arity) and, 
in the multi-sorted case, the actual sorts of function arguments and result.
The formal definition of the depth of a term is:\\
\m{d(\fa{f}{t}) \triangleq d(f) + d(\tup{t})}\\
\m{d(\tup{t}) \triangleq \mathop{max}\limits_i d(t_i)}\\
\m{d(x) \triangleq 1}\\
We assume the function \m{d(\cdot)} is given as a parameter for function and variable symbols - 
from here on we assume that the function has the value 1 for all function and variable symbols.

For the unsorted case, for the \textcolor{blue}{depth d} and a \textcolor{blue}{number c of constants}, the total number of terms is defined recursively as follows:\\
\m{G(1) = c}\\
\m{G(d+1) = \sum\limits_{f \in F_{>0}}  G(d)^{\arity{f}}}\\
Where \m{G(d)} is the number of terms up to size d and \textcolor{blue}{\m{F_{>0}} is the set of non-constant function symbols}.

To get an asymptotic upper bound, first we assume that all non-nullary functions are of the maximal arity m, and denote by f the number of non-constant functions - we get the following recurrence:\\
\m{G(1) = c}\\
\m{G(d+1) = c + f G(d)^{m}}\\
Which we can bound by \m{G(d)^{m} \geq c} for \m{d>1}:\\
\m{G(d+1) \leq (f+1)  G(d)^{m}}\\
And so we get:\\
\m{G(d) \leq (f+1)^{\sum\limits_{i=0}^{d-1} m^i}  c^{m^{d}}}
\m{= (f+1)^{\frac{m^d-1}{m-1}}  c^{m^{d}}}
\m{\approx \bigO{(f+1)^{m^{d-1}} c^{m^{d}}}}

We can see that the number of potential terms grows double-exponentially with the maximal arity and with depth, 
and that the number of constants is somewhat more significant than the number of non-nullary functions.
Most importantly, for a given finite signature and maximal depth, the number of terms is polynomial in the number of constants.
We expect proofs of many program properties to fit within very low depth restrictions (e.g. 3 or at most 4), 
and the signatures we have encountered do not usually include functions of arity more than 4.
In addition, as mentioned before, we can always reduce the maximal arity to be 2 (at the cost of deeper expressions).


\subsection{Relative depth}
The intuition behind enforcing depth limits is the search for \emph{small} proofs.
In an attempt to limit the size (total number of function predicate and variable symbol occurrences) in a proof DAG,
the depth of a term is a natural candidate. As we have seen in chapter \ref{chapter:ugfole}, 
many (but not all) of the examples that exhibited more than linear minimal sized proof used terms of a depth that was at least linear in the input size.
For any given program P, the minimal term depth \m{d_{min}(P)} that is sufficient to represent the program is the maximal depth of any ground term appearing in the program (definitions for general clauses follow later, but do not change the basic notions). 

Our objective is incremental verification, and hence we want to try and verify the program with some term depth limit \m{d_0}, 
simplify and trim the program according to the derived equalities (and clauses in general), and then, if there are still unproven assertions,
proceed with depth \m{d_1}, which could be, for example, \m{d_0+1.}
If we set \m{d_0 = d_{min}(P)}, we might miss the opportunity to discharge many simple assertions and simplify the program significantly before increasing the allowed term-depth - for example, consider the code in figure \ref{snippet4.1}:
\begin{figure}
\begin{lstlisting}[tabsize=1]
$\m{s:}$
	//Axioms
	assume $\m{\forall x \cdot P(x) \Rightarrow Q(x)}$
	assume $\m{\forall x \cdot P(x) \Rightarrow P(f(x,x))}$
$\m{n_0:}$
	//Prelude
	assume $\m{P(d)}$
	assume $\m{P(c)}$
$\m{n_1:}$
	assert $\m{\lnot Q(f(f(c,c),f(c,c)))}$ // negated $\m{Q(f(f(c,c),f(c,c)))}$
\end{lstlisting}
\caption{Relative term depth example}
\label{snippet4.1}
\end{figure}
here the minimal proof has a term depth of 4 as it must include the term \m{P(f(f(c,c),f(c,c)))}.
There are many terms of depth 4 in the signature that can be obtained by resolution or instantiation, including \m{P(f(f(d,d),f(d,d)))}.
Intuitively, \m{P(f(f(c,c),f(c,c)))} is nearer to the original problem than \\
\m{P(f(f(d,d),f(d,d)))} as \m{f(f(c,c),f(c,c))} appears in the original problem but the nearest term that includes d is d.
The idea is that the absolute depth (or in general, size) of terms in the proof is less important than the \emph{distance} of the term from terms that appear in the original program.
Before discussing the concept of distance we present another motivating example, taken from the tree-rotation code of AVL-trees and shown in figure \ref{snippet4.2}.

\begin{figure}
\begin{lstlisting}[tabsize=1]
$\m{s:}$
	//Heap axioms
	assume $\m{row_{1a}: \forall h,x,f,v,y,g \cdot x=y \lor rd(wr(h,x,f,v),y,g)=rd(h,y,g)}$
	assume $\m{row_{1b}: \forall h,x,f,v,y,g \cdot f=g \lor rd(wr(h,x,f,v),y,g)=rd(h,y,g)}$
	assume $\m{row_2: \forall h,x,f,v \cdot rd(wr(h,x,f,v),x,f)=v}$
$\m{n_0:}$
	//Prelude
	assume $\m{left \neq right}$ //$\m{\textcolor{gray}{left \neq right}}$
	assume $\m{this \neq null}$  //$\m{\textcolor{gray}{this_0 \neq null}}$
$\m{n_1:}$
	assume $\m{this.left \neq this}$  //$\m{\textcolor{gray}{rd(h_0,this_0,left) \neq this_0}}$
		...
	if (left != null)
		$\m{t_0:}$
		// $\m{\textcolor{gray}{rd(h_0,this_0,left) \neq null}}$
		if (right == null)
			$\m{t_1:}$
				// $\m{\textcolor{gray}{rd(h_0,this_0,right) = null}}$
			$\m{t_2:}$
				lr := left.right   //$\m{\textcolor{gray}{lr = rd(h_0,rd(h_0,this_0,left),right)}}$
				left.right := this //$\m{\textcolor{gray}{h_1 = wr(h_0,rd(h_0,this_0,left),right,this_0)}}$
				this := left       //$\m{\textcolor{gray}{this_1 = rd(h_1,this_0,left)}}$
				right.left := lr   //$\m{\textcolor{gray}{h_2 = wr(h_1,rd(h_1,this_1,right),left,lr)}}$
			$\m{t_a:}$
				assert right.right==null //$\m{\textcolor{gray}{rd(h_2,rd(h_2,this_1,right),right) \neq null}}$
\end{lstlisting}
\caption{Relative term depth example\\
Translated clauses are given in comments}
\label{snippet4.2}
\end{figure}

\begin{figure}
\begin{lstlisting}[tabsize=1]
		$\m{h_1 = wr(h_0,rd(h_0,this_0,left),right,this_0)}$
		$\m{this_1=rd(h_1,this_0,left)}$
				$\m{=rd(wr(h_0,rd(h_0,this_0,left),\mathbf{right},this_0),this_0,\mathbf{left})}$
				$\m{=rd(h_0,this_0,left)}$ ($\m{ \mathbf{row_{1b}:} \Leftarrow left \neq right}$)
		$\m{rd(h_1,this_1,right)=}$
				$\m{=rd(wr(h_0,rd(h_0,this_0,left),right,this_0),rd(h_1,this_0,left),right)}$
				$\m{=rd(wr(h_0,\mathbf{rd(h_0,this_0,left)},\mathbf{right},this_0),\mathbf{rd(h_0,this_0,left)},\mathbf{right})}$
				$\m{=this_0}$ ($\m{ \Leftarrow \mathbf{row_{2}}}$)
		$\m{h_2 = wr(h_1,rd(h_1,this_1,right),left,lr)}$
		$\m{rd(h_2,this_1,right)=rd(wr(h_1,rd(h_1,this_1,right),left,lr),this_1,right)}$
				$\m{=rd(wr(h_1,this_0,\mathbf{left},this_1),this_1,\mathbf{right})}$
				$\m{=rd(h_1,this_1,right)}$ ($\m{ \mathbf{row_{1b}:} \Leftarrow left \neq right}$)
				$\m{=rd(h_1,rd(h_0,this_0,left),right)}$ 
				$\m{=rd(wr(h_0,\mathbf{rd(h_0,this_0,left)},\mathbf{right},this_0),\mathbf{rd(h_0,this_0,left)},\mathbf{right})}$ 
				$\m{=this_0}$ ($\m{ \Leftarrow \mathbf{row_{2}}}$)
		$\m{rd(h_2,rd(h_2,this_1,right),right)}$
				$\m{=rd(h_2,this_0,right)}$
				$\m{=rd(wr(h_1,rd(h_1,this_1,right),\mathbf{left},lr),this_0,\mathbf{right}))}$
				$\m{=rd(h_1,this_0,right)}$ ($\m{ \Leftarrow \mathbf{row_{1b}:} left \neq right}$)
				$\m{=rd(wr(h_0,\mathbf{rd(h_0,this_0,left)},right,this_0),\mathbf{this_0},right))}$ 
				$\m{=rd(h_0,this_0,right)}$ ($\m{ \Leftarrow \mathbf{row_{1a}:} rd(h_0,this_0,left) \neq this_0}$)
				$\m{=null}$ 
\end{lstlisting}
\caption{Equational proof of program from \ref{snippet4.2} at\m{t_2}}
\label{snippet4.2_proof}
\end{figure}

Here \m{h_0,h_1,h_2} and \m{this_0,this_1} are the DSA versions of the heap and \lstinline|this|, respectively, \m{rd,wr} are map read and write operations, \m{left,right} are field-id constants and \lstinline|null| is a reference constant.
The minimal term depth for representing the program is 3.\\
In figure \ref{snippet4.2_proof} we show the part of an equation proof of the assertion that must be performed at \m{t_2} (according to the scoping rules). One can immediately see that the maximal depth of a term in the proof is 4.\\
The property of this proof is that all sub-terms that appear in the proof are provably equal to a term in the original program. \\
If we look at the equivalence class of each term  according to the equalities we can prove on this term, for example at \m{t_2}, the depth of the equivalence class of
\m{[read(h_0,this_0,left)]_2} is 2, as it has only one member.\\
The members of the EC \m{[read(h_0,read(h_0,this_0,left),right),lr]_2} have the depth 3 and 1, respectively, what should be the depth of the EC-node?

Assuming that we have other axioms that could generate other terms up to our depth limit, we want a depth measure that first searches for a proof using the existing terms and terms shown equivalent (so that they share the same EC-graph node), and only later try to generate new terms.
Our definition for relative term depth assigns each EC-node with the minimum of the depth of its EC-members.
The term \m{rd(h_2,rd(h_2,this_1,right),right)} appears only at the CFG-node \m{t_a,} and must participate in the proof.
The constants \m{h_2}, \m{this_1} appear at \m{t_2} but not at \m{t_1,} while \m{h_0}, \m{h_1}, \m{this_0} do not appear at \m{t_a}.
If we enforce minimal scoping, the above term could be represented at \m{t_1} as follows (repeated sub-terms are named)

\noindent\\
\m{rd(h_2,rd(h_2,this_1,right),right) = }\\
\indent\m{rd(s_6,rd(s_6,s_3,right),right)}

\noindent
\begin{tabular}{lll}
\m{s_1 \triangleq rd(h_0,this_0,left)} & \m{d=2}\\
\m{s_2 \triangleq wr(h_0,s_1,right,this_0)} & \m{d=3}\\
\m{s_3 \triangleq rd(s_2,this_0,left)}  & \m{d=4}\\
\m{s_4 \triangleq rd(s_2,s_3,right)}  & \m{d=5}\\
\m{s_5 \triangleq rd(h_0,s_1,right)}  & \m{d=3}\\
\m{s_6 \triangleq wr(s_2,s_4,left,s_5)} & \m{d=6}\\
\end{tabular}

\medskip

\noindent
This representation is quite large - the depth (both relative and absolute) is 8.
On the other hand, \m{h_1} ,\m{h_0}, \m{this_0} are out of scope at \m{t_a}, so that the parts of the proof shown above cannot be deduced in \m{t_a} (there is no term in the scope of \m{t_a} that can represent \m{h_0} - this would require extensionality).
Hence we see that if we look for a proof in increasing relative depth, we are forced to find the proof at \m{t_2}, at a relative depth of 1.

If we define the relative depth of a term at an CFG-node to be the minimal depth of a member in its EC according to the equalities implied (by the whole program) at the CFG-node, then we can only bound it from above for any intermediate step in our verification process, as new equalities deduced at a later stage can add new members to the EC with a lower depth. 
In light of this we only state an upper bound for relative depths unless we have found that the term is equivalent to an original program term, at which case it is of relative depth 1 as described below.

For a given verification state (that is, a given EC-graph \m{g_n} at each EC-node n), we denote the upper bound on the depth of a term as the minimal depth of its \emph{known} equivalence class, using \m{d^{\leq}_n(t)} as the upper bound for the depth of the term t at CFG-node n - for example:\\
\m{d^{\leq}_{t_3}(rd(h_2,rd(h_2,this_1,right),right)) = 1}\\
\m{d^{\leq}_{t_2}(rd(h_2,this_1,right)) = 2}\\
\m{d^{\leq}_{t_2}(rd(h_2,rd(h_2,this_1,right),right)) = 3}\\
However, in our system we can still prove the above program at a maximal term depth of 2:\\
Initially we can only add \m{rd(h_2,this_1,right)} to \m{t_2} at relative depth 2 (we describe the reasoning behind adding ground terms in the quantification chapter).\\
Instantiating the axioms using E-matching allows us to derive \\
\m{rd(h_2,this_1,right)=this_0} at \m{t_2} without introducing any new terms,
%,while introducing only the new terms \\ \s{rd(h_1,this_0,right),rd(h_0,this_0,right)} (both are initially of depth 2),
thus reducing the depth of the equivalence class:\\
\m{d^{\leq}_{t_2}(rd(h_2,this_1,right)) = d^{\leq}_{t_2}(this_0) = 1}\\
Now:\\
\m{d^{\leq}_{t_2}(rd(h_2,rd(h_2,this_1,right),right)) = 2}\\
Also
\m{d^{\leq}_{t_2}(rd(h_1,this_0,right)) = 2}\\
We derive:\\
\m{rd(h_2,rd(h_2,this_1,right),right) = rd(h_1,this_0,right)}, introducing only the new term \m{rd(h_1,this_0,right)} of depth 2.\\
Now also \\
\m{d^{\leq}_{t_2}(rd(h_0,this_0,right)) = 2}\\
So we can derive\\
\m{rd(h_1,this_0,right) = rd(h_0,this_0,right)}\\
Which triggers the propagation of \m{rd(h_0,this_0,right) = null} by the EC-graph mechanism.\\
All steps are performed on term-ECs of relative depth at most 2.

We use \textcolor{blue}{\m{o_n}} for the set of EC-nodes in \m{g_n} that represent \textcolor{blue}{terms in the original program} at the CFG-node n
(when a set of nodes is merged into one, the merged node is in \m{o_n} iff any of the nodes in the set was).
Our new depth definition would be, for an EC-node n in the EC-graph \m{g_n} at the CFG-node n, the least fixed point of:\\
\m{d^{\leq}_n(n) = \mathbf{if~} n \in o_n \mathbf{~then~} 1 \mathbf{~else~} min_{\fa{f}{t} \in n}(d^{\leq}_n{\fa{f}{t}})}\\
\m{d^{\leq}_n(\fa{f}{t}) = 1+\mathop{max}\limits_{i}(d^{\leq}_n{t_i})}\\
%\m{d_n(\fa{f}{t}) = \mathbf{if~} \terms{\fa{f}{t}} \cap \terms{\clauses{n}} \neq \emptyset \mathbf{~then~} 1 \mathbf{~else~} 1+max_{i}(d_n{t_i})}\\
The least fixed point is well defined as the function is monotonic.\\
We could, of course, select any non-negative constant for original program terms.

\textbf{Caveats:}
The local EC-node depth can be somewhat sensitive to the formulation of the VC, for example, if we take the above program and add a nullity check before each heap dereference, we get the program in figure \ref{snippet4.3}.

\begin{figure}
\begin{lstlisting}[tabsize=1]
		...
	if ($\m{c_1}$) //from if (left != null)  
		assume c_1
		assume left != null // $\m{\textcolor{gray}{rd(h_0,this_0,left) \neq null}}$
		if ($\m{c_2}$) //from if (right == null)  
			$\m{t_1:}$
			assume c_2
			assume right == null // $\m{\textcolor{gray}{rd(h_0,this_0,right) = null}}$
			$\m{t_2:}$
				$\m{t_{2a}:}$
				assert left != null   //negated $\m{\textcolor{gray}{rd(h_0,this_0,left)=null}}$
			$\m{t_3:}$
			lr := left.right   //$\m{\textcolor{gray}{lr = rd(h_0,rd(h_0,this_0,left),right)}}$
				$\m{t_{3a}:}$
				assert left != null   //negated $\m{\textcolor{gray}{rd(h_0,this_0,left)=null}}$
			$\m{t_4:}$
			left.right := this //$\m{\textcolor{gray}{h_1 = wr(h_0,rd(h_0,this_0,left),right,this_0)}}$
			this := left       //$\m{\textcolor{gray}{this_1 = rd(h_1,this_0,left)}}$
				$\m{t_{4a}:}$
				assert right != null   //negated $\m{\textcolor{gray}{rd(h_1,this_1,right)=null}}$
			$\m{t_5:}$
			right.left := lr   //$\m{\textcolor{gray}{h_2 = wr(h_1,rd(h_1,this_1,right),left,lr)}}$
			$\m{t_a:}$
			assert right.right==null //$\m{\textcolor{gray}{rd(h_2,rd(h_2,this_1,right),right) \neq null}}$
\end{lstlisting}
\caption{Relative term depth example 2}
\label{snippet4.3}
\end{figure}

Here, for a relative term depth bound of 2:\\
At \m{t_5} we must apply one axiom in order to even allow \\
\m{rd(h_2,rd(h_2,this_1,right),right))} to be represented.\\
At \m{t_4}, \m{rd(h_1,rd(h_1,this_1,right),right)} is only allowed after we establish that \m{rd(h_1,this_1,right)=this_0}.

\begin{figure}
\begin{lstlisting}[tabsize=1]
	...
	assume this $\neq$ null
	if ($\m{c_1}$) //from if (left.left != null)
		$\m{t_1:}$
		assume $\m{c_1}$
		assume left.left!= null   //$\m{\textcolor{gray}{rd(h_0,rd(h_0,this_0,left))\neq null}}$
		$\m{t_2:}$
		right := null //$\m{\textcolor{gray}{h_1 = wr(h_0,this_0,right,null)}}$
		//$\m{\textcolor{gray}{rd(h_1,this_0,left)}}=$
		//   $\m{\textcolor{gray}{rd(wr(h_0,this_0,\mathbf{right},null),this_0,\mathbf{left})}}$
		//   $\m{\textcolor{gray}{rd(h_0,this_0,left)}}$ ($\m{\textcolor{gray}{ \Leftarrow \mathbf{row_{1b}:} left \neq right}}$)
		//$\m{\textcolor{gray}{rd(h_1,rd(h_1,this_0,left),left)}}=$
		//   $\m{\textcolor{gray}{rd(wr(h_0,this_0,\mathbf{right},null),rd(h_0,this_0,left),\mathbf{left})}}$
		//   $\m{\textcolor{gray}{rd(h_0,rd(h_0,this_0,left),left)}}$ ($\m{\textcolor{gray}{ \Leftarrow \mathbf{row_{1b}:} left \neq right}}$)
		$\m{t_3:}$
		assert left.left!=null //negated $\m{\textcolor{gray}{rd(h_1,rd(h_1,this_0,left),left)=null}}$
		//$\m{\textcolor{gray}{rd(h_1,rd(h_1,this_0,left),left)}}$
\end{lstlisting}
\caption{Relative term depth limitation}
\label{snippet4.4}
\end{figure}

The program shown in figure \ref{snippet4.4} can only be proven with depth 3.\\
The term $\m{rd(h_0,rd(h_0,this_0,left),left)}$ requires a depth of 3 at \m{t_2,}
while if we were to merge \m{t_2} to \m{t_3} we would have a depth of 1, and similarly if we were to merge \m{t_2} with \m{t_1}.
Essentially, a term equivalent to the depth 3 term appears in both a successor and a predecessor of the CFG-node, and hence a user of the verification tool might expect a proof search at \m{t_2} to include the term.

\emph{Inheriting} the \emph{original} term-ECs from either the successor or the predecessor would mean that the size of \m{o_n} is proportional to the number of DSA versions of variables rather than to the number of variables in the program, which somewhat defeats the purpose of scoping. We have not found a more satisfactory way of determining the original set of term-ECs, and it remains as future work.

In light of the limitations of depth restrictions, we use these in combination with other methods to control the complexity. Note that most of the worst case complexity examples of chapter \ref{chapter:ugfole} are rejected using depth alone.


\subsection{Enforcing the depth restriction}
A simple technique to enforce the depth restriction in an EC-graph propagation algorithm is to apply the algorithm as is, calculate the depth for each node in the result, and trim away any EC-node (and potential-EC-node etc) as a post-process.

However, this would not be very efficient as we can have a quadratic (in the bound) number of EC-nodes that would be trimmed away (e.g. at a join, for E-unification the number is exponential) - hence we want to enforce the restriction at the construction level.

Our EC-graph join algorithm propagates equalities by adding sub-terms from predecessors top-down.
We build the \GFAs{} and EC-nodes top-down, starting with a known EC-node and adding \GFAs{} to it until we reach a known tuple - for example, when \lstinline|adding| the term c to the join n of \\
\m{\s{[a]_0,[f(a)]_0,[c,f(f(a))]_0}\sqcup\s{[a,b]_1,[f(a,b)]_1,[c,f(f(a,b))]_1}}, 
we begin by adding \m{[c]_n}, and descend the potential-relevant EC-node chain \m{f(.)},\m{f(f(.))},\m{f(f(a))} as long as both joinees agree on the function symbol and until we find an already existing EC-node or get stuck. 
We annotate each such potential-relevant EC-node with a \emph{reverse depth} which marks its distance from an existing EC-node (an existing node must have a relative depth within the bound).\\
In our example, \m{f(.)} has a reverse depth of 1, \m{f(f(.))} a reverse depth of 2 and \m{f(f(a))} a reverse depth of 3. 
It is easy to see that if we have reached the reverse-depth bound and not encountered an existing EC-node, any EC-node that is built using the \GT{} of maximal reverse depth will be out of the depth bound, and hence we do need to expand the chain further.
In addition, we do not need requests and \RGFAs{} that is deeper than the depth bound, as any response to such a request is not representable.

Applying all of the above ensures that, for each EC-graph \m{g_n,} the set of all representative terms of all \GFAs{} in all versions of \m{g_n} (throughout the verification process) is bounded by the limit calculated above.
Note that this still allows an exponential total number of possible states of \m{g_n}, as we can select any subset of the allowed \GFAs{} and any subset of the possible equalities on them, but in any given verification run only a polynomial number of these can appear. This is where we differ from SAT/SMT as, because all of our operations (\lstinline|add| and \lstinline|assume|) are monotonic, the size of any sequence of non-redundant operations (operations that actually modify \m{g_n}) is linear in the maximal size of the data structure (after applying non-redundant operations exhaustively we get an EC-graph with all possible \GFAs{} in one EC-node).

We have implemented size bound on construction both for the EC-graph and for E-unification.

\section{Size bounds for clauses}
For clauses, whether ground or non-ground, the bounds on terms give us an initial limit to the size of each clause, as each equality literal includes exactly two top terms and hence the number of possible literals is limited. This limitation is, however, usually insufficient for our purpose as the number of possible clauses is exponential in the number of possible literals which is exponential in the maximal depth.

For this reason we add a further bound that allows only a bounded number of literals per clause - a width bound. 
If the number of possible literals is l and the width bound is w, the unbounded number of possible clauses is \m{2^l} while the bounded number is \bigO{\m{l^w}}.

Some evidence is given in \cite{DBLP:journals/jacm/Ben-SassonW01} that restricting clause width can help us restrict the search for shorter proofs (although they deal with propositional proofs). The results in the paper are for general resolution and tree-like resolution, while the propositional fragment of superposition is equivalent to ordered resolution. 
Ordered resolution is known to have minimal width proofs that are wider than those of general resolution with some orders.
We use several propositional simplification inferences (that do not rely on ordering) in order to mitigate this, and have not encountered examples where clauses much wider than the input were needed in a proof. 

Clause width is easy to enforce in inferences as the number of literals of the conclusion is usually easily computable from that of the premises. Enforcing term-depth bounds for ground superposition is also relatively straightforward except when we use relative depth - in that case the relative depth of the EC of a super-term may be lower than the relative depth of a sub-term. 
Our algorithm for ground superposition, when generating the term-EC \termRepAt{t}{r}{p} from \termRepAt{t}{l}{p} and r by replacement, attempts to perform the replacement bottom up and aborts at the first step the depth bound is not satisfied.

\section{Derivation depth}\label{section:derivation_depth}
A proof system that maintains a set of clauses and updates it using generating and simplifying inferences simulates the search for a DAG-shaped proof. We define the derivation depth of each proof-DAG-node as the length of the longest proof-DAG-path from the clause to a leaf. 
If a clause can be derived in several different ways, the shortest derivation depth of these proofs is taken.\\
One of the main features of our system is looking at the derivation depth \emph{per-fragment} - each fragment being a set of inference rules. Given a proof-DAG that includes inferences from different fragments, a fragment F and a clause C derived in the proof, we count the maximum number of inferences in F along any path from a leaf to C. 

Given a set of clauses saturated for some derivation depth \m{d_F} in each fragment F, 
there is a marked difference in the cost of saturating the set for derivation depth \m{d_F+1} for each fragment F.
While fragments such as unit ground propagation can take almost linear time (depending on the indexing and search structures), 
in the non-ground superposition fragment, a clause with a maximal literal \m{x=y} can have twice as many superposition inferences with a clause C as there are sub-term occurrences in C. A clause such as \m{x=y \lor z=w} would have even more option.
For a clause width w and a maximal size (number of sub-term occurrences) s for a literal we could have at least \m{2w\times sw} inferences per pair of clauses in total.

Another factor is the interaction between the derivation depth and the different measures for terms and clauses - depth and width. In most inferences we consider, the maximal size measures of the result can be calculated easily from the premises, before performing unification or replacement. Relative depth makes this calculation an over-approximation, in which case we reject an inference only if an under-approximation shows the result is not within the bounds, and otherwise attempt the inference and abort it at the first indication that the result is not within the bounds.

Some fragments only decrease all size measures - for example, unit propagation. 
However, such a fragment might take a non-trivial time to saturate - for example, simplification by unit rewriting in the non-ground case requires non-trivial indexing and unification operations, even if it ensures that the result is smaller or equal to the original in all measures. Some fragments can also ensure an overall decrease in e.g. the tkbo order (which is well founded) of the whole problem (the result is smaller or equal to the original seen as a multiset of clauses ordered by tkbo), but it may take a large number of steps to saturate the fragment - for example, rewriting by ground unit equalities without EC-graphs (even ground) can take a quadratic number of steps to reach the normal form (e.g. rewrite rules \s{t_1\rightarrow t_2, ...,t_{n-1}\rightarrow t_n} and the term \m{f(t_1,.....,t_1)} with f of arity n). 
We separate a subset of the fragments that ensure a decrease in all measures (or no modification - we do not want fragments that permutate the state), and ensure a low polynomial (or log-linear) time to saturate (without a depth bound), and apply them eagerly.
These include the saturation of EC-graph propagation with the weak join, unit ground propagation and rewrite by unit equalities with EC-graphs (fully reduced rewrite system).

In light of the above, our strategy is to search for a proof of increasing derivation depth, but increasing the derivation depth of each fragment separately. We give higher priority to cheaper fragments and our system receives as a parameter the ratio between the derivation depth of different fragments.

The main technique in which our implementation enforces the derivation depth limitations described above is by dividing the proof process to passes, where each pass traverses the CFG in topological order (though see the implementation part for other possible evaluation orders),
and applies a per-pass subset of the fragments. Each pass must also ensure the propagation of the relevant clauses from predecessors. Our implementation usually performs propagation for most fragments together in order to reduce the overhead of requests.
This allows us to avoid annotating each clause with its derivation depth per-fragment.


\section{Summary}
We have seen how to obtain bounded fragments that limit the size and structure of clauses and terms that can appear in proofs, and how to combine such bounded fragments to get a predictable worst-case run-time. 
Limiting the shape of a proof in a local way (that is, limiting the shape of each clause or derivation step rather than e.g. the number of clauses or overall time-budget), allows users of a verification tool to predict the behaviour of the tool for a given program, and to adjust parameters accordingly. In addition, the results of the evaluation of each fragment is usable for other fragments, as the state of each fragment is representable as simply a set of clauses.\\
For several of the bounded fragments, the total worst-case run-time is still of a high polynomial degree, and so more work is needed in order to obtain a tractable and predictable verifier.
