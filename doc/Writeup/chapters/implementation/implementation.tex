\chapter{Implementation and experimental results}\label{chapter:implementation}
In this section we present some experimental results and some of the more interesting implementation issues.

\section{Order of evaluation}
Our algorithm specifies which clauses need to be propagated and which derivations have to be performed at each CFG-node in order to ensure completeness, but does not specify in which order derivations and propagations are performed.
In this section we discuss the effects the order of evaluation has on performance. 
We describe the experience we had with different orders and the choice we made for the algorithm.

There are two extremes for the order of evaluation, roughly in line with the difference between BFS and DFS (with regard to proof depth, not CFG structure): a BFS performs one step of one fragment at each CFG-node before performing the next step, while a DFS approach tries to discharge one assertion before moving on to the next. In general, we have found that neither of the extremes works well, and our approach is to search generally in BFS but to allow several cheaper fragments to saturate for each CFG-node before moving on to the next, including equality and heaps.

We give now some examples to the effects of evaluation order.
The basic operations a CFG-node can perform are saturation w.r.t. a given fragment, a propagation request to its immediate predecessors (that can include requests from its immediate successors) or the propagation of the response of its direct predecessors. We mark these operations as $n_1.frag$, $n_1.req$ and $n_1.prop$, respectively.\\
We use $sup$ for the fragment superposition, $req(x)$ to signify which term is requested and $prop(y)$ to signify which clauses are propagated. 

\begin{figure}
\begin{lstlisting}
$n_0$: x := 0
$n_1$: y := 1
$n_2$: z := 2
if (*)
	$n_3$: assert y!=z //negated $\comm{y=z}$
else
	$n_4$: assert x!=y //negated $\comm{x=y}$
\end{lstlisting}
\caption{Example for order of evaluation\\
We consider the fragments of ground superposition \\
where $z\succ y \succ x \succ 2 \succ 1 \succ 0$}
\label{snippet_7.1}
\end{figure}

The example program in figure \ref{snippet_7.1} shows that servicing requests eagerly (essentially DFS) incurs a significant overhead in CFG traversal. In our example, one possible order of evaluation traverses the CFG in topological order, and services requests eagerly - we get the following order of operations (we assume scoping is enforced to limit some requests - e.g. $n_3$ never requests $z$ as it is not in scope in $n_2$):\\
$n_0.sup - n_1.sup - n_2.sup - n_3.req(z) - n_2.prop(z=2) - n_3.prop(z=2) - n_3.sup$ now $n_3$ has derived $2\neq y$.\\
$n_3.req(y) - n_2.req(y) - n_1.prop(y=1) - n_2.prop(y=1) - n_3.prop(y=1) - n_3.sup$ Now $n_3$ has derived $2\neq 1$ and is eliminated.\\
$n_4.req(y) - n_2.prop(y=1) - n_4.prop(y=1) - n_4.sup - $\\
$n_4.req(x) - n_3.req(x) - n_2.req(x) - n_1.req(x) - $\\
$n_0.prop(x=0) - n_1.prop(x=0) - n_2.prop(x=0) - n_4.prop(x=0) - n_4.sup$\\
Note that $n_2$ has cached the result for $y$ and did not request it again from $n_1$.\\
We can see that CFG traversal can cause a lot of overhead even with caching of propagation results.

Another possible evaluation order that tries to reduce the overhead of CFG traversal traverses the CFG in reverse topological order only sending requests to predecessors, and then in topological order propagating responses and saturating. In the above example, the assertions are proven by two passes of request-propagate-saturate. 
While this evaluation order reduces the overhead of CFG traversal, it does not combine very efficiently with simplification derivations.

\begin{figure}
\begin{lstlisting}
$n_0$: assume $P(b) \lor Q(c)$
$n_1$: assume $\lnot P(a)$
$n_2$: assume $b=a$
...
$n_k$: assert $Q(c)$ //negated $\comm{\lnot Q(c)}$
\end{lstlisting}
\caption{Example for order of evaluation - simplifications\\
We assume $Q(c)\succ P(b)$ and $c\succ b \succ a$}
\label{snippet_7.2}
\end{figure}

For example, consider the program in \ref{snippet_7.2}, 
here, $n_k$ requests $Q(c)$ and the response $\{P(b) \lor Q(c)\}$ is propagated from $n_0$ through $n_1$,$n_2$, where it is simplified to $P(\mathbf{a}) \lor Q(c)$. If we follow the evaluation order of full CFG request-response cycles,
we propagate this clause to $n_k$, perform resolution and then we need another request-response cycle (requesting $\lnot P(a)$) to get a refutation.\\
However, if we allow eager servicing of requests, $n_3$ will request $Q(c)$, and when this request reaches $n_2$ the clause is simplified as above to $P(a) \lor Q(c)$, but now $n_2$ sends a request for $P(a)$ (both positive and negative) as it is a valid simplification literal.
If the request is serviced immediately $n_2$ can simplify the clause to $Q(c)$ which is then propagated to $n_k$ - we have reduced the number of CFG-traversals and also the size of propagated clauses. Similarly, as we have seen in chapter \ref{chapter:gfole}, we can avoid the propagation of some clauses entirely if we check for subsumption when propagating.

\subsubsection*{Heaps}\label{section:heaps}
Another fragment that requires a different strategy is the heaps fragment, and, more generally, any fragment that relies on pattern based instantiation.

\begin{figure}
\begin{lstlisting}
$n_0$: assume f$\neq$g
	$\comm{f\neq g}$
$n_1$: x.f:=5 
	$\comm{H_1 = H_0[x.f:=5]}$
$n_2$: x.g:=6 
	$\comm{H_2 = H_1[x.g:=6]}$
$n_3$: y.f:=7 
	$\comm{H_3 = H_2[y.f:=7]}$
if (x=y)
	$n_4$: assert y.g==6 
	$\comm{x = y}$
	$\comm{H_3[y.g]\neq 6}$
else
	$n_5$: assert y.g==old(y.g)
	$\comm{x \neq y}$
	$\comm{H_3[y.g]\neq H_0[y.g]}$
\end{lstlisting}
\caption{Example for order of evaluation - heaps\\
We use the DSA versions of the heap $H_0,H_1,H_2,H_3$\\
We use the fragment of UGFOLE plus the fragment of ground heaps\\
For superposition, the ordering is \\
$H_1 \succ g \succsep H_2 \succ y \succsep H_3 \succ H_0 \succ f \succ x$
}
\label{snippet_7.3}
\end{figure}

Consider the example in figure \ref{snippet_7.3}.
Assume we use superposition, with the following heap axioms:\\
$\forall h,r,u,r',u' \cdot (r\neq r' \lor u \neq u') \Rightarrow \underline{h[r'.u':=v][r.u]}=h[r.u]$ and\\
$\forall h,r,u,v \cdot \underline{h[r.u:=v][r.u]}=v$

\bigskip

\noindent
$n_1$ derives the following clauses:\\
$\forall r,u \cdot r=\m{x} \lor \underline{\m{H_1}[r.u]}=\m{H_0}[r.u]$ \\
$\forall r,u \cdot u=\m{f} \lor \underline{\m{H_1}[r.u]}=\m{H_0}[r.u]$ \\
$\underline{\m{H_1[x.f]}}=5$

\noindent
Similarly for $n_2$:\\
$\forall r,u \cdot r=x \lor H_2[r.u]=\underline{H_1[r.u]}$ \\
$\forall r,u \cdot u=g \lor H_2[r.u]=\underline{H_1[r.u]}$ \\
$\underline{H_2[x.g]}=6$ \\
And, together with the clauses of $n_1$ (after simplifications):\\
$\forall r,u \cdot r=x \lor \underline{H_2[r.u]}=H_0[r.u]$ \\
$\forall r,u \cdot u=f \lor u=g \lor \underline{H_2[r.u]}=H_0[r.u]$ \\
$H_2[x.f]=5$ 

\noindent
At $n_3$ we get:\\
$\forall r,u \cdot r=y \lor H_3[r.u]=\underline{H_2[r.u]}$ \\
$\forall r,u \cdot u=f \lor H_3[r.u]=\underline{H_2[r.u]}$ \\
$H_3[y.f]=7$ \\
And together with the propagated clauses:\\
$\forall r,u \cdot r=\m{y} \lor r=\m{x} \lor \underline{\m{H_3}[r.u]}=\m{H_0}[r.u]$ \\
$\forall r,u \cdot u=\m{f} \lor u=\m{g} \lor \underline{\m{H_3}[r.u]}=\m{H_0}[r.u]$ \\
$\forall r,u \cdot r=\m{x} \lor u=\m{f} \lor \underline{\m{H_3}[r.u]}=\m{H_0}[r.u]$ \\
$\m{x=y \lor \underline{H_3[x.f]}=6}$ \\
$\underline{\m{H_3}[x.g]}=6$ 

We can see that these derivations are not goal oriented - they are performed regardless of the assertions and entail the indexing and propagation of non-ground clauses. For example, the clause \m{H_3[x.g]=6} is not useful as the program never references the heap location \m{x.g} at any point after \m{H_3} is defined. In contrast, our heap fragment instantiates heap axioms only on demand - when a heap-read term occurs at a later CFG-node.

For the above example:\\
\m{n_4} requests \m{H_3[y.g],H_0[y.g]}, which causes \m{n_3} to instantiate:\\
\m{H_3[y.g]=H_2[y.g]}\\
Now $n_3$ requests \m{H_2[y.g],H_0[y.g]} and \m{n_2} instantiates:\\
\m{x=y \lor H_2[y.g]=H_1[y.g]}\\
\m{x \neq y \lor H_2[y.g]=6}\\
And then \m{n_2} requests \m{H_1[y.g],H_0[y.g]} and \m{n_1} instantiates:\\
\m{H_1[y.g]=H_0[y.g]}\\
Now $n_2$ derives, by ground superposition:\\
\m{x=y \lor H_2[y.g]=H_0[y.g]}\\
And \m{n_3} derives:\\
\m{x=y \lor H_3[y.g]=H_0[y.g]}\\
\m{x \neq y \lor H_3[y.g]=}

Note that without the above-mentioned requests, no instantiations occur, whether we use our heap domain or pattern based instantiation - e.g. the term \m{H_2[y.f:=7][y.g]} does not exist at \m{n_3} and cannot exist there because of scoping.
Note also the difference in complexity between the two approaches.

In terms of order of execution, it is easy to see that the above program can be verified in one backward-forward pass 
if the backward pass instantiates heap axioms and sends requests and the forward pass performs ground superposition (as implemented in our heap fragment). If we were to use only forward passes, even with eager request servicing, we would need four passes to prove the program, 
as each CFG-node requires instantiation in order to produce  the requests for its predecessors.

The above example suggests two modifications to eager request servicing - the first modification instantiates heap-axioms when propagating a request backwards and the second modification performs ground superposition on the propagated clauses when propagating the response to a request. With both modifications the above program can be proved in one forward pass.

We have implemented a dedicated heap fragment and experimented both with a single backward-forward pass and with the variants of eager request servicing.
In our experience, eager request servicing with instantiation on request-propagation greatly reduces the number of passes needed to prove a program, but the superposition step on response propagation is often expensive. Instead, we perform the forward pass that includes e.g. the ground equality fragment and a quantifier instantiation fragment with eager request servicing, and then perform one backward-forward pass of the entire CFG where the backward pass instantiates map axioms and propagates requests and the forward pass propagates responses and performs ground superposition only on equalities on map read terms.



\textbf{Sub-DAG saturation:}
A more DFS  oriented evaluation order we have experimented with saturates the CFG in topological order, but when saturating a goal node it repeats saturating the sub-CFG of the node's transitive predecessors (in topological order) until the whole sub-CFG is saturated, and only then moves on to the next CFG-node. 
In the program in figure \ref{snippet_7.1} this means that we saturate the sub-CFG $\{n_0,n_1,n_2,n_3\}$ before we even look at $n_4$.
In order to facilitate this order the traversal mechanism has to keep track of which CFG-nodes are not saturated, which is not trivial in cases such as the heap fragment above, where a request from a successor can enable more derivations.
We have found that, while this order allows the elimination of many goals on the first pass and hence produces a simpler CFG, the cost of CFG traversal dominates the execution time and so the overall performance is much worse.

\textbf{Our experience:}
We have experimented with several combinations of the above evaluation orders, where we saturate different combinations of fragments in different orders.

We have found that, in most cases, CFG traversal dominates the run-time and furthermore, the vast majority of requests have empty responses. The evidence we have seen for this is that, when we turn off the caching mechanism for any of the fragments, the algorithm grinds to a halt and usually does not finish even a single pass.

However, we have also found that sometimes servicing one simple request eagerly can allow a whole branch to be simplified or a goal node to be eliminated, rather than waiting until the whole CFG has performed one step. 
In light of the above we traverse the CFG in topological order and for each CFG-node collect all the requests for all fragments and service them eagerly, we continue by saturating and servicing eagerly only the simplification fragments, and propagate and cache an over-approximation of the requests, in order to reduce CFG traversal.

CFG-nodes responding to requests saturate the propagated clauses only w.r.t. simplifications (for which eager request servicing is applied). Each CFG-node remembers all the previous requests and if a new clause is derived anywhere that matches a previous request it is propagated in the next forward pass.
We apply one forward pass of instantiation or superposition (the more expensive fragments) followed by a pass of the cheaper fragments and then a garbage collection pass. 
A pass of garbage collection performs mark-sweep in reverse topological order where each CFG-node marks for its predecessor which of the previous requests is still relevant - this allows us to eliminate lemmas that were produced for a goal node that has been eliminated and not used by any other CFG-node.

\section{Implementation}
We have implemented the verification algorithm in C\#. We have implemented the fragments of UGFOLE, GFOLE, simple instantiations by E-matching (with a very simple and partial inference of patterns), heaps and, partially, superposition. 
We have also implemented some simple and incomplete integer handling (mostly by simplifying terms). 
We have implemented scoping and depth and width bounds at the construction level (that is, for all EC-graph operations and for unification and E-unification). We have not implemented interpolation but we calculate the ordering for superposition (used also for heaps) as described for interpolation and hence, while not complete, superposition is geared more towards eliminating terms that are not in scope in successors. We have also used the ordering for superposition as a hint when inferring patterns for instantiation, by preferring maximal (in $\succ$) terms as patterns.

\subsubsection*{Pre-processing}
Our implementation takes a Boogie program after it has been simplified by Boogie to include only \lstinline|assume| and \lstinline|assert| statements (not in CNF form, and the CFG has non-binary joins, but no loops) and then performs several transformations and simplifications to bring the program to our required shape - including:
\begin{itemize}
\item Type grounding - Boogie uses polymorphic types with type quantifiers, but in our experience it is often the case that the relevant type instances can be determined a-priori, so we instantiate only the relevant type-instances of axioms.
\item Function inlining and elimination of unused or useless declarations.
\item Branch determinization - by adding a branch condition propositional literal for non-deterministic branches.
\item Extraction of Boolean functions, similar to \cite{Kotelnikov:2016:VF:2854065.2854071} (Boogie supports Boolean typed expressions in any position, as in SMT-lib, while standard FOL supports Boolean functions only under the predefined Boolean connectives).
\item CFG transformation to enforce a well branching program (binary branches and matching joins) and also several CFG simplifications
\item CFG unit propagation - a simple analysis that treats each literal as its (unique) string representation, and performs unit-propagation along the CFG, calculating the set of unit clauses that hold at each CFG-node and simplifying clauses accordingly - this simple analysis can often remove up to half of the assertions. While the assertions removed are mostly simple ones, removing them allows us to simplify the CFG.
\item Scope calculations - we take the initial set of clauses at each CFG-node and calculate the minimal scope needed to ensure completeness even with quantifiers, as described in chapter \ref{section:scoping:node_scope}.
\item DSA map calculations - for Boogie maps (often used to represent heaps) we track the DSA chain for each map typed term that occurs in the program all the way to the root-node. We use the unit clauses at each node to find heap updates and also detect CFG-nodes where it is likely that a frame rule has been applied (that is, when a DSA chain is broken for a map but there is another map of the same type that has a DSA chain), we then detect possible frame axioms (that equate two map-reads of the same type with free variables) and optionally instantiate the potential frame rule for the relevant pair of maps if found. We can also optionally track each map-read in the initial program through its DSA chain and add the relevant terms that form its DSA chain, in order to reduce CFG-traversals later.
\end{itemize}

\subsubsection*{Main algorithm}
The data structure at each CFG-node is based on an EC-graph (as in chapter \ref{chapter:ugfole}) and a set of clause-ECs (as in section \ref{section:appendix:gfole_ECGraph}) over the nodes of the EC-graph.\\
In order to enable the sharing of EC-nodes between different CFG-nodes, each EC-node only includes references to EC-nodes in its sub-graph (that is, only for sub-terms), and any references for super-terms (such as set of super-terms with a given function) is kept per CFG-node.
For example, if $a=b$ holds in two consecutive CFG-nodes $n_1,n_2$ and both contain the EC-node for the terms $f(a),g(f(a))$ but at $n_2$ also $g(f(a))=c$ holds, they will share the EC-sub-graph $[a,b],[f(a),f(b)]$ but not the EC-nodes for $g(f(a)),c$.\\
Each EC-node caches information such as depth, tkbo ordering information and transitive sub-terms.\\
We keep a central repository of all EC-nodes to ensure that there is exactly one instance of each EC-sub-graph.

\textbf{Equality:}
The handling of equality is by the algorithm from chapter \ref{chapter:ugfole}, which interleaves congruence and transitive closure with equality propagation. This algorithm is used to add term-ECs to the EC-graph, propagate term-ECs from predecessors, synchronize with a predecessor after it had been updated and adding new equalities (e.g. derived by other fragments).

The algorithm, as described, does not ensure a unique object (meaning the same EC-sub-graph is represented by the same object in all EC-nodes in which it appears). The algorithm works on a temporary copy of the sub-EC-graph that is modified, and once it is done (\lstinline|mainLoop| terminates), a unique representation is found for each modified EC-node.

The search for unique objects for EC-nodes is performed by first splitting the simplified EC-graph to strongly connected components (SCCs) (with the \GFA{} edges directed to sub-terms) and then processing them in topological order (Each not-modified EC-node is included in the first connected component).\\
For each component, we define a total order on its EC-nodes and number them accordingly, and then produce a unique string that represents the SCC - we keep a map of these strings for all existing SCCs in the system.
For example, the simplified EC-graph \s{[f(a),g(h(b))],[h(b)]} which references the unmodified EC-nodes \s{[a],[b]} has one SCC that includes the entire graph. In order to find an order between the ECs \m{[f(a),g(h(f(a)))],[h(f(a))]}, we first select the \GFAs{} that only reference EC-nodes from previous SCCs - in this case this is only \m{f(a)}. On this \GFA{} we define a total order as we have a total order on existing EC-nodes by order of creation, which always respects the topological order on SCCs. 
We order the ECs with such \GFAs{} by their minimal \GFA{} and number them accordingly - so \m{[f(a),g(h(b))]} is assigned the number 0.
We proceed by ordering the next set of \GFAs{} that reference only unmodified \emph{and numbered} ECs - in our case the \GFA{} \m{h(f(a))}assigns the number 1 to \m{[h(f(a))]}. We continue assigning numbers and ordering \GFAs{} until all ECs in the SCC are assigned a number.
We are guaranteed to assign numbers to all ECs as each EC is guaranteed to represent at least one term (we do not search for a unique representative for the empty \GTs{} - they are only used as witnesses for completeness).
We are now ready to produce our unique string - we represent the ECs in order and within each EC the \GFAs{} in order. For each unmodifed EC-node we have a pre-calculated unique string (we use an integer index) for each new EC we use the numbers - so in our case the string is
\lstinline|[f(s$_{\m{[a]}}$),g(1)],[h(0)]| where \lstinline|s$_{\m{[a]}}$| is the pre-calculated string for the unmodified EC $\m{[a]}$.

\textbf{Clauses:}
Clauses are represented as clause-ECs over the EC-graph of the EC-node in which they occur (so a clause may have more than one instance if it occurs in two CFG-nodes with a different EC-graph).
Clauses are simplified eagerly (e.g. unit clauses are propagated eagerly, duplicate literals are eliminated and a clause with opposing literals is eliminated). Clause-ECs are updated only once the congruence closure is saturated for the EC-graph and are indexed (for instantiation or superposition) only before a pass that saturates the relevant fragment - so a clause can be simplified or even eliminated before it is indexed.

\subsubsection*{Garbage collection}
As a merged EC-nodes is replaced by a new node, and as many clauses and literals are discarded by the simplification rules, the algorithm tends to create a lot of garbage. In addition, each CFG-node maintains some caches for requests from transitive successors, which are used to propagate new clauses that match previous requests without needing to re-issue the request.
When some of these successors are simplified or eliminated, some of these requests become irrelevant (e.g. if we had the clause \m{C \lor \underline{b}=a} and requested \m{b}, and later we derived \m{C} and the clause is subsumed). We perform a mark-sweep pass that traverses the CFG in reverse topological order and removes unnecessary elements in each CFG-node, marking the still relevant requests in direct predecessors. We also mark all used term- and clause-ECs and discard those that are no longer used. 
We have experimented also with reference counting but the overhead is quite significant and support for cyclic EC-graphs is only partial.


\section{Experimental results}



As we did not implement support for rationals and only a limited support for integers, and as our implementation is written more as a test-bed for experiments than as an optimized verifier, we do not expect to be competitive with mature tools, such as Z3, for smaller examples, as optimizations and especially the optimization of the pre-processing stages dominates performance on such examples. 
Hence, we looked for examples that take Boogie+Z3 long time to prove, and that are not arithmetic intensive - which meant, mostly, heap intensive programs. 
We also tried to collect examples from different front-ends to Boogie as they tend to generate VCs of different styles. 

We have collected Boogie programs from Dafny, Chalice, VCC. We show results only for the long running examples.
For Chalice, we have written an implementation of an AVL tree with all the standard operations and full specification, 
where the elements of the tree are modeled as a sequence typed ghost field. Dafny and VCC programs tend to use set axioms.
We have also experimented with some of the long-running VCs used in the thesis \cite{Rdch2011}, 
which encode a \emph{stereotype}, which is a collection of reusable specifications for heap-manipulating data structures.

\subsection*{Experiments}
As we have not implemented a sufficient integer fragment, and as most interesting programs include at least some integer reasoning, our tool cannot, in itself, verify many interesting programs. Instead, we run our tool on the Boogie program and, if we cannot completely verify the program, we dump an SMTLib version of the state at various stages of the algorithm and compare the run-time of an SMT solver on the original Boogie SMTLib output and the output from our tool (with the run-time of the tool up to that point added). 

We have experimented with different orders for applying the various fragments and also with different values for the following parameters:
\begin{itemize}
	\item Maximal term depth
	\item Maximal clause width (number of literals)
	\item For heaps: whether to perform ground superposition on instances of map axioms eagerly, or only create the instances
	\item For superposition: whether to derive only ground clauses
\end{itemize}

\subsection*{SMT solver}
For programs that our tool could not handle on its own, we have used Z3 as the SMT solver back-end for the remaining (simplified) proof obligations.
The logic needed for most interesting examples (examples that take a significant time for Boogie to prove) is at least UFLIA (quantifiers, uninterpreted functions and linear integer arithmetic).
According to SMTCOMP (e.g. \cite{CDW14}), the only provers that support this logic are CVC4 (\cite{DBLP:conf/cav/BarrettCDHJKRT11}), CVC3, Z3 (\cite{DBLP:conf/tacas/MouraB08}), veriT (\cite{DBLP:conf/cade/2009}) and recently Vampire\_smt (apparently based on \cite{Voronkov14}). We have tried running the Boogie output, and the output of our tool with CVC4, CVC3, Z3, veriT and also with Beagle (\cite{DBLP:conf/cade/BaumgartnerBW15}) as it is a superposition based prover that also supports some numerical reasoning using hierarchical superposition and supports SMTLib natively.

Of the above provers, only Z3 was able to handle any of the interesting problems (with a run-time of more than a few seconds).
We believe the reason is as follows:
Boogie itself was built with Z3 in mind and uses in its output some Z3 specific attributes - specifically, it uses weights for quantifiers in axioms to direct the relative depth of instantiation.
A common problem for pattern-based quantifier-instantiation is that of matching loops - for example, an axiom 
$\forall x \cdot P(x) \Rightarrow P(f(x))$ with the pattern $P(x)$, combined with the axiom $P(c)$ will produce an infinite number of instances $\{P(f^n(c)) \mid n \in \mathbb{N}\}$. A matching loop can also involve several axioms. 
While some of the cases of matching loops can be detected by the SMT solver, there is no way to find a maximal instantiation depth for such axioms (As FOL and FOLE are only semi-decidable), hence some prioritization of instances is needed.
SMT solvers sometimes use a heuristic that tries to direct quantifier instantiation away from such matching loops, 
assigning to each term-EC a measure that roughly describes how many quantifier instantiations were needed to produce that term.
This measure is called the weight or depth or generation of a term (there are different strategies and measures used in different provers - e.g. the depth of \cite{DBLP:journals/jacm/DetlefsNS05}). The measure often also depends on other parts of the SMT-solver, to control the mix between decisions and quantifier instantiation. For our example, such a measure could be 0 for $P(c)$, 1 for $P(f(c))$, 2 for $P(f(f(c)))$ and so on. Prioritizing quantifier instantiation to the instances where the pattern is matched by a lower measure allows fairer instantiation of axioms and avoiding generating too many instances of axioms with such matching loops (some axioms can also produce a finite but exponential number of instances (e.g. using permutations), which some of the measures can also help to avoid).

For a given path in the Boogie VC of a program, the minimal instantiation depth needed to prove an assertion at the end of the path that contains a heap read can be up to the number of heap updates on the path - in some of our examples, this is in excess of a hundred heap-updates per-path (especially Chalice uses a shadow permission heap which is updated often). Allowing other axioms to instantiate to such a depth will make most solvers extremely inefficient. 

Z3 allows annotating each quantifier with a weight that is used as a coefficient for determining the measure of terms introduced in quantification, as a function of the measure for terms used in instantiation. 
Boogie assigns a weight of zero to the map read-over-write axioms, allowing them to instantiate to any depth, because heap updates that are generated from Boogie heap-writes are guaranteed to be oriented to the CFG (that is, a DSA version of the heap can only be an update of an earlier DSA version), except for some very unusual cases. Without the instantiation measure, an SMT solver has little chance to guess that a specific quantified axiom has to instantiated much more than others. We have examined the number of axiom instances used by Z3 in the proof search and, as expected, it is dominated by map axioms for most heap-manipulating examples (sequence axioms also rank high in some examples).

We have tried some examples with Z3 with the weight annotation removed and Z3 could not prove any but trivial examples.
CVC4 supports several quantifier instantiation strategies in addition to using patterns, including model-based quantifier instantiation (\cite{DBLP:conf/cav/GeM09}) and conflict guided quantifier instantiation (\cite{DBLP:conf/fmcad/ReynoldsTM14}). We tried running several versions of CVC4 with different quantifier instantiation strategies, but none could prove any of the non-trivial examples. 
CVC4 is leading the UFLIA and UFNIA categories in the latest SMTCOMPs and hence we expect that it could handle many of the examples, but this would require further work with the generation of the SMTLib output. 

A possible solution is to encode heaps using the native array theory of SMTLib. The problem is that Boogie uses polymorphic maps with dependent types - the type returned by a heap read depends on the type of the field. 
Boogie uses an encoding that adds type parameters as terms and uses type coercion functions (as described in \cite{DBLP:conf/tacas/LeinoR10}). Boogie supported using a native array theory of Z3, but this support is deprecated. 

However, for some examples that CVC4 could not prove for the reasons above, we found that running CVC4 on the output of our tool allowed finding a refutation. This was the case only for simpler examples, and we suspect the reason is that our tool derived most of the necessary instances of heap axioms. In addition, the CVC4 performance on such example was better when we ran our tool for more passes, up to a point.

In light of the above, our results are biased towards the particulars of Z3. 


\subsection*{Encoding the SMTLib output}
We have found that the encoding of the SMTLib output has a very significant effect on the performance of Z3, which is to be expected as Z3 uses E-matching and lazy CNF conversion.
We kept our encoding as similar to the Boogie encoding as possible, where the only significant parameter is which part of the information calculated by the tool is transferred to the SMT solver.

Our minimal encoding includes the simplified CFG (with any node shown infeasible removed). We use the pre-CNF-conversion form of of the formulae describing the VC and only simplify them using our derived information, as eager CNF conversion has a significant negative effect on the performance of Z3.
We use the EC-graph and the calculated unit clauses at each CFG-node to simplify the formula at each CFG-node. Simplification is by rewriting terms and literals to normal forms.
Normal forms for terms are the minimal representative of an EC in the EC-graph of the relevant CFG-node as per the superposition ordering - essentially if we have derived the fact that a certain ground term equals an integer or Boolean constant at some program point, any occurrence of the term at a formula at that program point is replaced by the constant, and the equality between the term and the constant is added to the set of formulae at that program point. After rewriting terms to normal form by the EC-graph we use the derived unit clauses at a CFG-node to simplify the original formula at the CFG-node, apply propositional simplifications and eliminate any formula that evaluates to true. We do not use non-unit clauses as they may contain literals added for CNF conversion, which affect Z3 performance badly. In theory, one could reverse the CNF conversion (at least for ground clauses) as we know which literals were introduced during conversion, and this remains as future work.

We have also experimented with an encoding that does not apply the above simplifications, and an additional encoding that adds equality information for all map reads (hence essentially encoding our interpretation of the map axioms) and a further encoding that includes all of our derived information (although in CNF form).
The full encoding with all derived information always performs very badly - we believe that the reason is a combination of the CNF conversion and the introduction of new terms, which affect pattern based quantifier instantiation.

\subsection*{Configuration}
The experiments were run on an Intel Core i7 2600K quad core processor with 16GB of memory, running Windows 7 64bit.
All experiments use only a single core.

The first set of experiments (run as a Boogie optimizer) was run twice, with some of the experiments performed several times. We have found that the run-time of our tool is very consistent (for longer running examples) and so the other experiments were performed only once.

\subsection*{Use as a Boogie optimizer}
The overall performance of the tool when run as a pre-process for Boogie is shown in table \ref{fig_results_overall}.
We used our tool running one pass of saturating the UGFOLE fragment and instantiation, 
including the heaps fragment but without ground superposition on map clauses.
We only show results for long running examples that verify in Z3. We run all examples with a time-out of 600 seconds as this captures the Boogie run-time of most examples with some margin.
Most results show an improvement in the total run-time. The Dafny examples are somewhat problematic, as explained below. We note that, on one hand, we only included the simpler fragments our tool can handle, as the implementation of the more advanced fragments is not yet competitive, but on the other hand, our tool, pre-processor and back-end are not highly optimized.


\textbf{Encoding of the SMTLib output:}
As noted above, our SMTLib back-end encodes the VC quite similar to Boogie - we use nested \lstinline|let| expressions to encode control flow, and use unit clauses derived by our algorithm to simplify axioms.
We do not use the CNF clauses as CNF conversion is harmful to Z3 performance.

For examples originating in Dafny, our SMTLib back-end faces a problem.
Our pre-processor removes type quantification by enumerating the possible types for each quantifier, 
allowing us to use typed versions of the axioms rather than using type coercion as done by Boogie.
The problem is that the frame axioms of Dafny are encoded using type quantification in order to apply to heap accesses of all field types, but the type variable is not part of the pattern for the frame axiom, 
and hence in the output of our back-end each frame axiom that is instantiated once in the Boogie output is instantiated several times in our output (in fact, in all the Dafny examples, there are five instances for five different field types). We have found that, for the Boogie output of all the Dafny examples we have inspected, the frame axioms are instantiated many times (up to millions of times) even when there are very few distinct instances - evidently Z3 fails to learn clauses constructed with instances of this axiom. 
The direct outcome is that our pre-processor amplifies this problem by eliminating type quantification - the output of the back-end of our tool, even just after the pre-processor, performs much worse in Z3 simply because many more (non distinct) instances of the frame axiom are generated.
We have been able to mitigate this effect somewhat by removing one axiom of the Dafny preamble - this axiom
encodes the transitivity of heap succession - the transitivity of the \lstinline|$\$$HeapSucc(a, b)| predicate which is used on the left hand side of all frame axioms. This axiom was not needed by any of the examples we have inspected,
and has a strong negative effect on Z3 performance regardless of our tool (the original VC for the \lstinline|AVLTree| for Dafny took almost 3000 seconds to prove with the axiom, and around 400 seconds without).
However, for the \lstinline|TreeBarrier| example, we still suffer from the elimination of type quantification.
It is theoretically possible to reverse this elimination in the back-end, at least for the original axioms, but we have not implemented this.

\floatstyle{plain}
\restylefloat{table}


\begin{table}
\centering
\scriptsize
\begin{tabular}{|l||r|r|r||r|r|}
\hline
                           &  \textbf{time(s)} &   & & \\
\hline
\textbf{VC}                &  Boogie & Optimizer & Z3 & \% \\

\hline
\textbf{Chalice} &&&&\\
\hline
 AVLTree.insert                 &  29 &   9 &  16 &  86\%\\
\hline
AVLTree.pruneMax               &  31 &   6 &  18 &  77\%\\
\hline
AVLTree.rebalanceLR            & 273 &  21 & 233 &  93\%\\
\hline
AVLTree.rebalanceRL            &  TO &  21 & 207 & \\
\hline
AVLTree.remove                 &  62 &  17 &  36 &  85\%\\
\hline
\textbf{Dafny} &&&&\\
\hline
AVLTree.InsertRecursion              & 405 &  17 &  TO &  \\
\hline
AVLTreeGF.InsertRecursion          &  79 &  18 &  41 &  74\%\\
\hline
B4.Map.Remove                                      &  12 &   3 &   1 &  33\%\\
\hline
TreeBarrier.Node.barrier                           &  11 &  10 &  40 & 454\%\\
\hline
\textbf{Stereotypes   } &&&&\\
\hline
PIP.AddToEnd                               &  11 &   1 &  10 & 100\%\\
\hline
PIP.Node.acquire                          &  64 &   3 &  35 &  59\%\\
\hline
\end{tabular}
\caption{Results of running the verifier as an optimization pre-process\\
The column titled Boogie is the baseline Boogie performance with Z3 as a backend. 
The Optimizer column is the time taken for our tool run as an optimizer, 
the Z3 column is the time taken for Z3 on the optimized output of our tool 
and the last column is percentage of time taken by our tool and Z3 from the baseline time of only Z3.
TO means timeout - more than 600 seconds.}
\label{fig_results_overall}
\end{table}


\subsection*{Scoping}
In order to quantify the effects of scoping on performance and completeness, we run several of the larger benchmarks both with and without scoping. We run each benchmark for four passes, in the order equalities, heaps, instantiation, heaps.
In most cases, the number of assertions proven per pass was the same.
The results are shown in table \ref{fig_results_scoping}. 
The effects of scoping are almost universally beneficial, and the effect on completeness (in the limited fragment which we used) seems less significant.

\begin{table}
\centering
\scriptsize
\begin{tabular}{|l|l||r|r|r|r||r|r|}
\hline
            &     & \multicolumn{4}{c||}{\textbf{time(s)}}    &   \textbf{\#proven}    & \textbf{$\Delta$} \\
 \textbf{VC}&   \textbf{pass} \#                     & 1  & 2   &   3 &   4 &  & \\
\hline
\textbf{Chalice}  &&&&& && \\
\hline
AVLTree.rebalanceLR &  scoping    & 24	 & 81	 &  92 &  151  & 9 &  \\
                 &  no scoping & 27	 & TO	 &     &       & 1 & \\
\hline
AVLTree.insert   &  scoping    & 10  &	20 &	21 &	31   & 24 &  75\% \\
                  &  no scoping & 11	 & 57	 &  62 &  122  & 24 &\\
\hline
FactorialActor   &  scoping  & 41 &  45    &   65    &   80&  80 &  \\
                  &  no scoping &  44 & TO &         &     &  1  &\\
\hline
\textbf{Dafny} &&&&&&&\\
\hline
BFS              &  scoping    & 6  &	7 &	7 &	8    & 9  & \\
                  &  no scoping & TO	 & 	 &   &    & 0 & \\
\hline					
B4               &  scoping    & 3  &	3 &	3 &	4  & 4  & 0\%\\
                  &  no scoping & 3	& 3 & 4 & 4  & 4 &  \\
\hline					
AVLTree.InsertRecursion   &  scoping    & 18 &	26 &	31 & 39 & 105 & 20\%\\
                          &  no scoping & 21 & 31	 &  39 &  49 & 105 & \\
\hline
\textbf{VCC}      & & & & & & &\\
\hline					
vscomp2010\_p1\_v2.sum\_max  &  scoping    & 8 &	9 &	10 & 11 & 4 & 0\%\\
                          &  no scoping & 8 & 10 & 10 & 11 & 4  & \\
\hline
vscomp2010\_p3\_v0.find     &  scoping    & 9 &	10 & 10 & 11 & 2 & 9\%\\
                          &  no scoping & 9 & 10 & 11 & 12 & 2  & \\
\hline
VCC3.RBTree.left\_rotate   &  scoping    & 21 &	30 & 33 & 38 & 7 & 15\%\\
                          &  no scoping & 22 & 35 & 39 & 45 & 7  &\\
\hline
VCC2.RBTree.tree\_insert   &  scoping    & 114 &	131 & 138 & 149 & 23 & 30\%\\
                          &  no scoping & 133 & 167 & 184 & 211 & 23  & \\
\hline
VCC2.RBTree.right\_rotate   &  scoping    & 23&	34 & 37 & 49 & 38 & 36\%\\
                          &  no scoping & 29 & 49 & 59 & 77 & 39 & \\
\hline
%RBTree.left\_rotate&  time(s) & 7  &	24 &	33 &	64 & 147   \\
          %&                  &  no scoping & 4	 & 6	 &  10 &  12 &	19  \\
%\hline					
%					& AVLTree.Insert   &  time(s) & 26 &	51 &	149 & & \\
%          &                  &  \#proven  & 74 &	98 &  99 & & \\
\end{tabular}
\caption{The effect of scoping on the run time and number of assertions proven.\\
We show the total time taken for the first n passes.\\
The last column shows the improvement for four passes - empty when our tool timed out without scoping.\\
The \lstinline|tree_insert| took about 75 seconds in the pre-processing stage that modifies the CFG-node scope so that a total order can be defined, because of an inefficiency in the implementation which we believe is fixable.}
\label{fig_results_scoping}
\end{table}

\subsection*{Term depth restriction}
In order to quantify the effects of depth restrictions, we run our tool on VCs with many assertions that do not require arithmetic (the examples we have found come from Dafny, Chalice and VCC - other tools seem to produce less assertions per VC) and compared the number of assertions proven for each term depth bound. We did not count the assertion proven in our pre-processing steps, as they are not affected by the term depth bound. 
The results are shown in table \ref{fig_bounded_depth}. 
While we do not have many samples for which we could quantify the effect of term-depth, the results do hint that a gradual increase of term depth can be useful for proving a subset of the assertions, even for programs we cannot completely prove. 
In addition, for the vast majority of the benchmarks, running with a very low term depth (1 or 2), the tool terminated in a short time, regardless of whether it diverged for higher depths.
A use case for the tool would be running very low term depth without a time-out and then attempting to verify with a higher term-depth with a time-out. Marking each proven assertion with the depth needed to prove it can help the user to understand how changes in the program affect the proof complexity for assertions.

While we need to implement arithmetic in the tool in order to get full results, and more samples are needed, 
the above results, and some inspection of not proven assertions, suggest that program proofs often do not need very large terms. Running the tool with increasing term depth and a time-out might produce more useful results than running a monolithic VC with the same time-out, if not all assertions are proven.

\begin{table}
\captionsetup{width=0.8\textwidth}
\caption{The effect of term depth bounds on run-time and number of assertions proven.}
\label{fig_bounded_depth}
\centering
\scriptsize
\begin{tabular}{|l||l||r|r|r|r|r|}
\hline
                         &                    & \textbf{depth}   &    &    &    &     \\
\hline
\textbf{VC}              &                    & 1  & 2   &   3 &   4 &   5  \\
\hline
\textbf{Chalice}  & &&&&&\\
AVLTreeNode.init &  time(s) & 4	 & 6	 &  10 &  12 &	19  \\
          &  \#proven  & 7	 & 23	 &  38 &	38 &  38  \\
\hline
AVLTree.insert   &  time(s) & 3  &	10 &	18 &	27 &	35  \\
                  &  \#proven  & 1	 &   6 &	20 &	20 &	20  \\
\hline
FactorialActor   &  time(s) & 21 & 	65 &	91 & 196 &	419 \\
                 &  \#proven  & 0	 &  25 &  77 &  99 & 	111 \\
\hline
\textbf{Dafny}  & &&&&&\\
\hline
BFS              &  time(s) & 7  &	10 &	11 &	11 & 11   \\
                  &  \#proven  & 2	 &   6 &   7 &   7 &  7   \\
\hline					
AVLTree.InsertRecursion   &  time(s) & 26 &	51 &	149 & TO & \\
                  &  \#proven  & 74 &	98 &  99 & & \\
\hline
\textbf{VCC}  & &&&&&\\
\hline
VCC2RedBlackTree.left\_rotate&  time(s) & 7  &	24 &	33 &	64 & 147   \\
\hline
                  &  \#proven  & 17	 &  36 &   38 &   39 &  39   \\
\hline
VCC2RedBlackTree.tree\_insert&  time(s) & 17  &	113 &	142 &	148 & 200   \\
\hline
                  &  \#proven           & 6	 &  23 &   24 &   24 &  24   \\
\hline
%VCC3RedBlackTree.left\_rotate&  time(s) & 19  &	89 &	33 &	64 & 147   \\
%                             &  \#proven & 0	 &  7 &   38 &   39 &  39   \\
%\hline
%\hline					
%					& AVLTree.Insert   &  time(s) & 26 &	51 &	149 & & \\
%          &                  &  \#proven  & 74 &	98 &  99 & & \\
\end{tabular}
\end{table}


\subsection*{Additional experiments}
We have also experimented with varying the maximal clause width, but experiments showed little difference even with no width bound - only very few examples derived very wide clauses and they were often not needed for the proof. We did encounter a few examples that diverged after applying several superposition passes, when clause width was not restricted.

Our tool reports the ratio of sharing for \GTs{} and \GFAs{} and also clause- and atom-ECs.
The sharing ratio for \GTs{} and \GFAs{} (the number of CFG-nodes where the \GT{} resp. \GFA{} occurs) was consistently between 4 and 6 for the vast majority of examples. For atom-ECs it was lower and for clause-ECs yet lower. This result supports the design of our EC-graph data structure, but does not support the additional overhead of atom- and clause-ECs.
