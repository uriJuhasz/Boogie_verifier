	\subsection{Heap}
	Programs using the heap have always posed a challenge for the analysis and proving of programs.
	Potential aliasing between two different heap access paths is a generally undecidable problem and for many interesting subsets and over-approximations of the problem computationally very hard.
	As proofs of bigger programs are often intractable and whole program techniques do not scale, a modular approach is followed by most tools.
	In order for a modular verifier to be able to reason about method calls without using the method body, some kind of aliasing specification must be provided/inferred - so several aliasing specification methodologies have been developed, such as ownership types ~\cite{DBLP:conf/ecoop/ClarkeNP01}, ~\cite{DBLP:books/sp/Muller02}, separation logic ~\cite{DBLP:conf/lics/Reynolds02}, permissions ~\cite{DBLP:conf/sas/Boyland03}, dynamic frames ~\cite{DBLP:conf/fm/Kassios06} ~\cite{DBLP:conf/lpar/Leino10}.
	Heaps are often represented as a map from locations (+field id) to locations/values and so we can expect to benefit from specialized reasoning procedures about maps (which some theorem provers have).
	In the SSA/DSA form of the program, every assignment to the heap, and, in general, every entry/return to/from a method or loop body, would create a new instance of the whole heap, and specify which values are changed and which are preserved using some kind of framing.
	The verification conditions generated from a program + specification in one of these methodologies includes several elements:
	\begin{itemize}
		\item Original programming language executable heap access - that is, direct reads and writes of the heap by the program - these can be in general handled quite well by a verifier as long as no loops are involved.
		Here it is often pretty obvious when a case split is needed over whether two heap paths alias or not, but different methodologies have different ways to encode lack of aliasing, so getting that information might be costly.
		\item Assertions about specific heap locations - pre- and post- conditions, loop invariants and method call/return specifications can all introduce assumptions about a specific heap location, without ever assigning it - thus some heap locations are accessed without ever being assigned, only have some assumed information - when this location is of a reference type and is later dereferenced this can imply many potential aliases and thus is more complicated
		\item Assertions that quantify over heap locations - for example, dynamic frames may assume the preservation of all values in a given frame over a method call - these are harder to reason about than the above, but tend to appear in a more specific form in framing assumptions to specify non-modification.
		Assumptions about actual modification - e.g. that all nodes in an integer list have the value 0 - are often encoded into a "folded" predicate, and thus only appear when the predicate for a specific node is unfolded.
		When this is not the case, the verification condition has to refer to sets of heap locations as in dynamic frames (e.g. nodes of the list) or to have some predicate over these locations (e.g. the "shadow" permission heap of Chalice) - both these cases require more specialized reasoning in order to infer non-aliasing or values/properties for a given heap location - in the case of dynamic frames that would operations over sets of locations, and in the case of Chalice we would need to correlate permission arithmetic on the permission heap with accesses to the same (or sometimes other) locations on actual heap.
		\item The worst case would be axiomatized uninterpreted operators over maps (aside from single location read/write) - but as far as we know these are not very common.	
		\item Some programming methodologies would generate proof obligations and axioms that use transitive closure to describe sets of heap locations - e.g. all nodes belonging to a list/tree.
		These require either a tool that can handle transitive closure (as, for example, TVLA) or some translation into FOL, either by the translator doing some reasoning or by partial FOL axiomatization of transitive closure.
		\end{itemize}
		In the first case some specialized handling of heaps can make proving more efficient as, for example, even a completely deterministic, executable program with full aliasing information, calculating the value of a given heap location may require many applications of the map read-over-write axioms (for any write on an execution path leading to that read, including branch/join of results).
		In the second case, if some property was assumed on a a heap location at SSA version i, and we require that property at SSA version $j>i$, we would need to apply at least $j-i$ map-read-over-write axioms, recognize the relevant ground assumption (possibly requiring intermediate lemmas) and then promote it (usually free with congruence closure) up to our desired SSA version.
		As there may be many assumed properties, most of which would not be required by the proof for all SSA relevant versions of the heap location, we try to generate these lemmas on demand - that is, when we determine that e.g. $heap[x,value]>0$ would be useful, we would apply the map-read-over-write axioms for the index tuples $[x,value]$, and at each stage try to find a proof for the $>0$ property.
		In this case, as discussed later, it might be advantageous to generate this property as a lemma for all intermediate SSA versions as, for example at a branch point, the lemma might be used by more than one successor state.
		In the third case, some VCG tools will already generate the axiom instances instantiated for the relevant heap locations, while others will just assume a quantified assumptions and let the prover figure out which locations are affected - in the second case it might be advantageous to treat these quantifier instantiations with a priority over other quantifier instantiations as usually determining the current value of a heap location means digging deep into the heap's SSA/DSA path - for example Boogie encoded into Z3 disables the term generation limitation on quantifier instantiation in order to be able to dig as deep as needed into the heap history.
		In the fourth case, it would seem that some specialized procedure would be in order, potentially encoded by the VCG tool.
